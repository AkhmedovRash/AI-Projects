This code implements a chatbot model based on the Encoder-Decoder LSTM architecture. It loads the dialog file, pre-processes the data and prepares it for training the model. The model is then built and compiled with the chosen optimizer and loss function. The model is trained on inputs and outputs, and then inference models are created to use the model in inference mode. You can then ask questions and receive answers from the chatbot.
The necessary modules and libraries are imported, including the file upload module, numpy, modules and classes from TensorFlow.keras for creating a neural network model, a module for working with YAML files, and a module for working with Google Drive.
The drive.mount() function mounts Google Drive in order to access the files.
A file with dialogs in YAML format is loaded using the open() function.
The conversations are extracted from the file and the number of question-answer pairs is displayed.
Dialogues are processed: questions and answers are extracted from conversations, extra characters are removed, and answers are framed with '<START>' and '<END>' tokens.
A tokenizer is created that collects a frequency dictionary from questions and answers.
The size of the dictionary is determined and a fragment of the dictionary is displayed.
Questions and answers are converted to sequences of tokens and padded with zeros to the maximum length.
The input and output data are loaded to train the model.
Model architecture is created using the Keras functional API.
The model is compiled with the chosen optimizer and loss function.
The model is trained on input and output data using the fit() function.
Inference models are created to use the model in inference mode.
The strToTokens() function converts a string into a sequence of tokens for input to the model.
In a loop, you can ask questions and receive answers from the chatbot using inference models.
можешь написать описание этого кода и дать ему названия?
from google.colab import files # модуль для загрузки файлов в colab
import numpy as np # библиотека для работы с массивами данных

from tensorflow.keras.models import Model, load_model # из кераса подгружаем абстрактный класс базовой модели, метод загрузки предобученной модели
from tensorflow.keras.layers import Dense, Embedding, LSTM, Input # из кераса загружаем необходимые слои для нейросети
from tensorflow.keras.optimizers import RMSprop, Adadelta # из кераса загружаем выбранный оптимизатор
from tensorflow.keras.preprocessing.sequence import pad_sequences # загружаем метод ограничения последовательности заданной длиной
from tensorflow.keras.preprocessing.text import Tokenizer # загружаем токенизатор кераса для обработки текста
from tensorflow.keras import utils # загружаем утилиты кераса для one hot кодировки
from tensorflow.keras.utils import plot_model # удобный график для визуализации архитектуры модели

import yaml # импортируем модуль для удобной работы с файлами
from google.colab import drive
drive.mount('/content/drive')
corpus = open('/content/drive/MyDrive/Диалоги(рассказы)_censored.yml', 'r') 
document = yaml.safe_load(corpus) 
conversations = document['разговоры'] 
print('Количество пар вопрос-ответ : {}'.format(len(conversations)))
print('Пример диалога : {}'.format(conversations[123]))
corpus.close()
questions = list() 
answers = list() 
for con in conversations: 
  if len(con) > 2 : 
    questions.append(con[0]) 
    replies = con[1:]
    ans = '' 
    for rep in replies: 
      ans += ' ' + rep 
    answers.append(ans) 
  elif len(con)> 1: 
    questions.append(con[0]) 
    answers.append(con[1]) 
answersCleaned = list()
for i in range(len(answers)):
  if type(answers[i]) == str:
    answersCleaned.append(answers[i]) 
  else:
    questions.pop(i) 
answers = list()
for i in range(len(answersCleaned)):
  answers.append( '<START> ' + answersCleaned[i] + ' <END>' )
print('Вопрос : {}'.format(questions[200]))
print('Ответ : {}'.format(answers[200]))
tokenizer = Tokenizer()
tokenizer.fit_on_texts(questions + answers) # загружаем в токенизатор список вопросов-ответов для сборки словаря частотности
vocabularyItems = list(tokenizer.word_index.items()) # список с cодержимым словаря
vocabularySize = len(vocabularyItems)+1 # размер словаря
print( 'Фрагмент словаря : {}'.format(vocabularyItems[:50]))
print( 'Размер словаря : {}'.format(vocabularySize))
tokenizedQuestions = tokenizer.texts_to_sequences(questions) 
maxLenQuestions = max([ len(x) for x in tokenizedQuestions]) 
paddedQuestions = pad_sequences(tokenizedQuestions, maxlen=maxLenQuestions, padding='post')
encoderForInput = paddedQuestions[:5000]
print('Пример оригинального вопроса на вход : {}'.format(questions[100])) 
print('Пример кодированного вопроса на вход : {}'.format(encoderForInput[100])) 
print('Размеры закодированного массива вопросов на вход : {}'.format(encoderForInput.shape)) 
print('Установленная длина вопросов на вход : {}'.format(maxLenQuestions)) 
tokenizedAnswers = tokenizer.texts_to_sequences(answers) 
maxLenAnswers = max([len(x) for x in tokenizedAnswers])
paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers, padding='post')
decoderForInput = paddedAnswers[:5000] 
print('Пример оригинального ответа на вход: {}'.format(answers[100])) 
print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[200])) 
print('Размеры раскодированного массива ответов на вход : {}'.format(decoderForInput.shape)) 
print('Установленная длина ответов на вход : {}'.format(maxLenAnswers)) 
tokenizedAnswers = tokenizer.texts_to_sequences(answers)
a = tokenizedAnswers[:5000]
for i in range(len(a)) : 
  a[i] = a[i][1:] 
paddedAnswers = pad_sequences(a, maxlen=maxLenAnswers , padding='post')
oneHotAnswers = utils.to_categorical(paddedAnswers, vocabularySize) 
decoderForOutput = np.array(oneHotAnswers) 
print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[100][:21]))  
print('Пример раскодированного ответа на выход : {}'.format(decoderForOutput[100][4][:21])) 
print('Размеры раскодированного массива ответов на выход : {}'.format(decoderForOutput.shape))
print('Установленная длина вопросов на выход : {}'.format(maxLenAnswers)) 
encoderInputs = Input(shape=(None , ))  
encoderEmbedding = Embedding(vocabularySize, 200 , mask_zero=True) (encoderInputs)
encoderOutputs, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)
encoderStates = [state_h, state_c]
decoderInputs = Input(shape=(None, )) 
decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs) 
decoderLSTM = LSTM(200, return_state=True, return_sequences=True)
decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)
decoderDense = Dense(vocabularySize, activation='softmax') 
output = decoderDense (decoderOutputs)
model = Model([encoderInputs, decoderInputs], output)
model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')
print(model.summary()) 
model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=20) 
def makeInferenceModels():
  encoderModel = Model(encoderInputs, encoderStates) 
  decoderStateInput_h = Input(shape=(200 ,)) 
  decoderStateInput_c = Input(shape=(200 ,))
  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] 
  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)
  decoderStates = [state_h, state_c] 
  decoderOutputs = decoderDense(decoderOutputs) 
  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)
  return encoderModel , decoderModel
def strToTokens(sentence: str):
  words = sentence.lower().split()
  tokensList = list() 
  for word in words: 
    tokensList.append(tokenizer.word_index[word])
  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')
encModel , decModel = makeInferenceModels() 
for _ in range(3): 
  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
  emptyTargetSeq = np.zeros((1, 1))    
  emptyTargetSeq[0, 0] = tokenizer.word_index['start']
  stopCondition = False 
  decodedTranslation = '' 
  while not stopCondition : 
    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
    sampledWordIndex = np.argmax( decOutputs[0, 0, :]) 
    sampledWord = None 
    for word , index in tokenizer.word_index.items():
      if sampledWordIndex == index: 
        decodedTranslation += ' {}'.format(word)
        sampledWord = word 
    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
      stopCondition = True 
    emptyTargetSeq[0, 0] = sampledWordIndex 
    statesValues = [h, c] 
  print(decodedTranslation[:-3]) 
model.compile(optimizer=Adadelta(), loss='categorical_crossentropy')
model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=512, epochs=30) 
def makeInferenceModels():
  encoderModel = Model(encoderInputs, encoderStates) 
  decoderStateInput_h = Input(shape=(200 ,))
  decoderStateInput_c = Input(shape=(200 ,)) 
  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] 
  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)
  decoderStates = [state_h, state_c] 
  decoderOutputs = decoderDense(decoderOutputs) 
  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)

  return encoderModel , decoderModel
def strToTokens(sentence: str):
  words = sentence.lower().split()
  tokensList = list() 
  for word in words: 
    tokensList.append(tokenizer.word_index[word])
  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')
encModel , decModel = makeInferenceModels() 
for _ in range(3): 
  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
  emptyTargetSeq = np.zeros((1, 1))    
  emptyTargetSeq[0, 0] = tokenizer.word_index['start']
  stopCondition = False 
  decodedTranslation = '' 
  while not stopCondition : 
    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
    sampledWordIndex = np.argmax( decOutputs[0, 0, :]) 
    sampledWord = None 
    for word , index in tokenizer.word_index.items():
      if sampledWordIndex == index: 
        decodedTranslation += ' {}'.format(word)
        sampledWord = word 
    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
      stopCondition = True 
    emptyTargetSeq[0, 0] = sampledWordIndex 
    statesValues = [h, c] 
  print(decodedTranslation[:-3]) 
model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=512, epochs=50) 
def makeInferenceModels():
  encoderModel = Model(encoderInputs, encoderStates) 
  decoderStateInput_h = Input(shape=(200 ,))
  decoderStateInput_c = Input(shape=(200 ,)) 
  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] 
  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)
  decoderStates = [state_h, state_c] 
  decoderOutputs = decoderDense(decoderOutputs) 
  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)
  return encoderModel , decoderModel
def strToTokens(sentence: str):
  words = sentence.lower().split()
  tokensList = list() 
  for word in words: 
    tokensList.append(tokenizer.word_index[word])
  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')
encModel , decModel = makeInferenceModels() 
for _ in range(10): 
  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
  emptyTargetSeq = np.zeros((1, 1))    
  emptyTargetSeq[0, 0] = tokenizer.word_index['start']
  stopCondition = False 
  decodedTranslation = '' 
  while not stopCondition : 
    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
    sampledWordIndex = np.argmax( decOutputs[0, 0, :]) 
    sampledWord = None 
    for word , index in tokenizer.word_index.items():
      if sampledWordIndex == index: 
        decodedTranslation += ' {}'.format(word)
        sampledWord = word 
    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
      stopCondition = True 
    emptyTargetSeq[0, 0] = sampledWordIndex 
    statesValues = [h, c] 
  print(decodedTranslation[:-3]) 
